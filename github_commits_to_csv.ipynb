{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export GitHub Commits to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2023\n",
    "org = 'examind-ai'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'token {os.getenv(\"GITHUB_ACCESS_TOKEN\")}',\n",
    "    'Accept': 'application/vnd.github.v3+json',\n",
    "}\n",
    "\n",
    "def fetch_page_items(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    items = response.json()\n",
    "    next_page_url = None\n",
    "\n",
    "    # Check if there is a 'Link' header and parse for next page URL\n",
    "    if 'link' in response.headers:\n",
    "        links = response.headers['link'].split(', ')\n",
    "        next_link = [link for link in links if 'rel=\"next\"' in link]\n",
    "        if next_link:\n",
    "            next_page_url = next_link[0].split(';')[0].strip('<>')\n",
    "\n",
    "    return items, next_page_url\n",
    "\n",
    "def fetch_all_items(url):\n",
    "    all_items = []\n",
    "    next_url = url\n",
    "    while next_url:\n",
    "        items, next_url = fetch_page_items(next_url)\n",
    "        all_items.extend(items)\n",
    "\n",
    "    return all_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching commits for 'examind-web'...\n",
      "Fetching commits for 'examind-moderator'...\n",
      "Fetching commits for 'examind-firebase'...\n",
      "Fetching commits for 'PrairieLearn'...\n",
      "Fetching commits for 'pl-demo-course'...\n",
      "Fetching commits for 'examind-question-builder'...\n",
      "Fetching commits for 'examind-demo-widget-tester'...\n",
      "Fetching commits for 'blackboard-export-viewer'...\n",
      "Fetching commits for 'froala-source'...\n",
      "Fetching commits for 'examind-froala'...\n",
      "Fetching commits for 'qti-export-viewer'...\n",
      "Fetching commits for 'examind-blockly'...\n",
      "Fetching commits for 'json-2-csv'...\n",
      "Fetching commits for 'ltijs-demo-server'...\n",
      "Fetching commits for 'examind-io'...\n",
      "Fetching commits for 'mui-submodule-to-top-level-imports'...\n",
      "Fetching commits for 'examind-access-code'...\n",
      "Fetching commits for 'examind-access-code-cf'...\n",
      "Fetching commits for 'examind-lti-proxy'...\n",
      "Fetching commits for 'ltijs-firestore'...\n",
      "Fetching commits for 'ltijs-firestore-scheduler'...\n",
      "Fetching commits for 'react-floater'...\n",
      "Fetching commits for 'react-joyride'...\n",
      "Fetching commits for 'firebase-tools-cross-service-rules'...\n",
      "Fetching commits for 'examind-openai'...\n",
      "Fetching commits for 'examind-api-proxy'...\n",
      "Fetching commits for 'integrity-advocate'...\n",
      "Fetching commits for 'compromised-questions'...\n",
      "Fetching commits for 'owp-gpt-pipeline'...\n",
      "Fetching commits for 'owp-gpt'...\n",
      "Fetching commits for 'owp-gpt-pinecone'...\n",
      "Fetching commits for 'owp-gpt-image-resize'...\n",
      "Fetching commits for 'owp-gpt-devops'...\n",
      "Fetching commits for 'owp-gpt-slideshow'...\n",
      "Fetching commits for 'bitnami-docker-moodle'...\n",
      "Fetching commits for 'lexical-sentence-node'...\n",
      "Fetching commits for 'parse-minutes'...\n",
      "Fetching commits for 'multiple-answers'...\n",
      "Fetching commits for 'lexical-question-builder'...\n",
      "Fetching commits for 'lexical-fill-in-the-blank'...\n",
      "Fetching commits for 'examind-essay'...\n",
      "Fetching commits for 'ltijs-data-decrypter'...\n",
      "Fetching commits for 'rd-autograding'...\n",
      "Fetching commits for 'examind-collaboration-detection'...\n",
      "Fetching commits for 'getexamind'...\n",
      "Commits saved to 'C:\\Users\\Johnny\\Downloads\\2023_examind-ai_commits.csv'.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "filename = f'{year}_{org}_commits.csv'\n",
    "filepath = os.path.join(os.path.expanduser('~'), 'Downloads', filename)\n",
    "\n",
    "\n",
    "if os.path.exists(filepath):\n",
    "    raise FileExistsError(f\"The file '{filepath}' already exists. Please remove it first.\")\n",
    "\n",
    "since = datetime(year, 1, 1).isoformat() + 'Z'\n",
    "until = datetime(year + 1, 1, 1).isoformat() + 'Z'\n",
    "\n",
    "repos_url = f'https://api.github.com/orgs/{org}/repos'\n",
    "\n",
    "# Define the CSV header\n",
    "fields = ['Repository', 'SHA', 'Author', 'Date', 'Message']\n",
    "\n",
    "repos = fetch_all_items(repos_url)\n",
    "\n",
    "with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(fields)  # Write the CSV header\n",
    "\n",
    "    for repo in repos:\n",
    "        repo_name = repo['name']\n",
    "        print(f\"Fetching commits for '{repo_name}'...\")\n",
    "    \n",
    "        commits_url = f'https://api.github.com/repos/{org}/{repo_name}/commits?since={since}&until={until}'\n",
    "        commits = fetch_all_items(commits_url)\n",
    "\n",
    "        for commit in commits:\n",
    "            sha = commit['sha']\n",
    "            author = commit['commit']['author']['name']\n",
    "            date = commit['commit']['author']['date']\n",
    "            message = commit['commit']['message']\n",
    "            csvwriter.writerow([repo_name, sha, author, date, message])\n",
    "\n",
    "print(f\"Commits saved to '{filepath}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
